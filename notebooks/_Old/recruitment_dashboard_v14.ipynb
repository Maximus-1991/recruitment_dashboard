{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Check why max_id doesnt work\n",
    "\n",
    "# Write .py functions\n",
    "# Dockers\n",
    "\n",
    "# Retrieve all keys in dictionary that do not contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workable API\n",
    "import requests\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date\n",
    "import mysql.connector\n",
    "\n",
    "\n",
    "headers={'Authorization': 'Bearer 229aa3876e4fc4447460a13da7f57d1be4111202e1d56d4d0231fb932c1e7cd1'}\n",
    "url = 'https://jdriven.workable.com/spi/v3/'\n",
    "\n",
    "#r_jobs = requests.get(url+'jobs.json', headers=headers)\n",
    "#r_stages = requests.get(url+'stages.json', headers=headers)\n",
    "#r_cand = requests.get(url+'candidates.json', headers=headers)\n",
    "\n",
    "#By default results are limited to 50. The limit can by updated via the request parameter limit\n",
    "#The value specified cannot be more than 100.\n",
    "#cand_dict['paging']\n",
    "#{'next': 'https://jdriven.workable.com/spi/v3/candidates?limit=50&since_id=e48b5d'}\n",
    "\n",
    "#since_id: string. Returns results with an ID greater than or equal to the specified ID\n",
    "\n",
    "#paging indicates what the next webpage is. \n",
    "# Since_id indicates that the requested data is taken before the since_ID\n",
    "# In other words, the next page starts with since_D\n",
    "# {'next': 'https://jdriven.workable.com/spi/v3/candidates?limit=100&since_id=f36d27'}\n",
    "# Limit indicates the number of IDs retrieved before the since_ID\n",
    "\n",
    "#max_id\n",
    "# Get max_id, limit=100 to get the 100 latest entries\n",
    "# Then use the first entry of that request for the following request, using since_Id=first entry of the first request\n",
    "\n",
    "# I want to retrieve the last ID\n",
    "# Maybe can retrieve using created_after and setting this to last week. If it cannot find anything, using another week back in time\n",
    "#'https://jdriven.workable.com/spi/v3/candidates?limit=100&created_after=2019-09-01T13:29:49Z}'\n",
    "\n",
    "#Timestamp parameters input format\n",
    "#Supported input formats for the timestamp fields created_after & updated_after are:\n",
    "#ISO8601 e.g. 20150708T115616Z\n",
    "#Unix time (e.g. 1436356721)\n",
    "\n",
    "\n",
    "# MySQL DB:\n",
    "#IP address: 127.0.0.1\n",
    "#Port: 3306\n",
    "#DB: recruitment_dashboard\n",
    "#user name: root\n",
    "#PW: maartens1991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maarten\\Anaconda3\\envs\\ams\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "from collections import Sequence\n",
    "from itertools import chain, count, tee\n",
    "\n",
    "def locate_element(data,look_up_elem):\n",
    "    '''\n",
    "    Function to locate the exact location of a an element in a data structure\n",
    "    '''\n",
    "    data_orig = data\n",
    "    loc_list = []\n",
    "    \n",
    "    #### Step 1: Create loop: while look_up_elem not in loc_list\n",
    "    while look_up_elem not in loc_list:\n",
    "\n",
    "        data = data_orig\n",
    "        if loc_list != []:\n",
    "            for location in loc_list:\n",
    "                data = data[location]     \n",
    "        \n",
    "        #### Step 2: Create loop for each element in data. This element needs to be appended to loc_list if element is found in (sub-levels of) this element\n",
    "        # Combine step 4 and 5 in one function. Function is to flatten the data and check if look_up_elem is present in data. If element is found, return loc_list\n",
    "        def check_branche(data):\n",
    "                    #### Step 2: Check if look_up_element is present on 1st level of data\n",
    "            if look_up_elem in data:\n",
    "                loc_list.append(look_up_elem)\n",
    "                return loc_list\n",
    "\n",
    "            #### Step 3: If element not present on 1st level, filter out strings and integers from data. Method is different for different data types\n",
    "            # Note: data_tuple = () (will be problematic, as you cannot append elements to a tuple). We may be able to add items from tuple to list as tuple is also a Sequence\n",
    "\n",
    "            # Define data_elements\n",
    "            if type(data)==dict: \n",
    "                data_elements = list(data.keys())\n",
    "            elif type(data)==list: \n",
    "                data_elements = list(range(len(data)))\n",
    "            # elif type(data)==tuple: \n",
    "                #data_elements = list(range(len(data)))\n",
    "\n",
    "            else:\n",
    "                return \"Element not present\"\n",
    "                      \n",
    "            for element in data_elements:\n",
    "\n",
    "                data_to_check = data[element]\n",
    "\n",
    "                # Define data_dict, data_list and data_tuple\n",
    "                if type(data_to_check)==dict:\n",
    "                    data_dict = data_to_check\n",
    "                    data_list = []\n",
    "                    data_tuple = ()\n",
    "                elif type(data_to_check)==list:\n",
    "                    data_dict = {}\n",
    "                    data_list = data_to_check\n",
    "                    data_tuple = ()\n",
    "                elif type(data_to_check)==tuple:\n",
    "                    data_dict = {}\n",
    "                    data_list = []\n",
    "                    data_tuple = data_to_check\n",
    "                elif type(data_to_check)!=dict and type(data_to_check)!=list and type(data_to_check)!=tuple:\n",
    "                    continue\n",
    "                else:\n",
    "                    return \"Error\"\n",
    "\n",
    "                #### Step 5: Enter while loop (is within the for loop of step 4). From the filtered data obtained in step 3, divide the different elements into its data type. Then, flatten type(data) data type first and then the other two data types\n",
    "                # When look_up_elem is found, append element to loc_list and return loc_list\n",
    "                while data_dict != {} or data_list != [] or data_tuple !=():\n",
    "                    # Flatten dictionary and check if element is present on any of the levels and add list elements to data_list\n",
    "                    # After first round, if any elements were added to data_dict, go through these added elements\n",
    "                    while data_dict != {}:\n",
    "                        if look_up_elem in data_dict:\n",
    "                            loc_list.append(element)\n",
    "                            return \n",
    "                            \n",
    "                        data_dict_temp = {}\n",
    "                        # Filter the elements in data_dict\n",
    "                        for key,value in iter(data_dict.items()):\n",
    "                            if type(value)==dict:\n",
    "                                data_dict_temp.update(value)\n",
    "                            elif type(value)==list:\n",
    "                                data_list.append(value)\n",
    "                            #elif type(value)==tuple:\n",
    "                            #    test_tuple\n",
    "                            # to check if tuple is also a sequence, can also use chain(element) for this\n",
    "                            else:\n",
    "                                \"Element is string or integer\"\n",
    "                        data_dict = data_dict_temp\n",
    "\n",
    "                    # After data_dict is (temporarily) exhausted, go through data_list\n",
    "                    while data_list != []:\n",
    "                        if look_up_elem in data_list:\n",
    "                            loc_list.append(element)\n",
    "                            return loc_list\n",
    "\n",
    "                        data_list_temp = []\n",
    "                        # Filter the elements in data_dict\n",
    "                        for item in data_list:\n",
    "                            if type(item)==dict:\n",
    "                                data_dict.update(item)\n",
    "                            elif type(item)==list:\n",
    "                                for i in item:\n",
    "                                    data_list_temp.append(i)\n",
    "                            else:\n",
    "                                \"Element is string or integer\"\n",
    "                        data_list = data_list_temp\n",
    "\n",
    "                    # After data_list is (temporarily) exhausted, go through data_tuple\n",
    "                    #while data_tuple !=():\n",
    "                        # Flatten tuple, check if element is present on any of the levels and add dictionary and list elements to data_dict or data_list\n",
    "                        #pass\n",
    "\n",
    "            if look_up_elem not in loc_list:\n",
    "                return \"Element not Found\"\n",
    "        check_branche(data)\n",
    "    \n",
    "    return loc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def change_name_duplicate_keys(d):\n",
    "    d_no_dupes = copy.deepcopy(d)\n",
    "    all_keys = {}\n",
    "    all_keys_temp = {}\n",
    "    list_check = []\n",
    "    duplicate_remover_dict = d_no_dupes  \n",
    "    while duplicate_remover_dict !={}:\n",
    "        dict_temp = {}\n",
    "        no_dupes = 0\n",
    "        for key,value in iter(duplicate_remover_dict.items()):\n",
    "            if type(value)==dict and value !={}:\n",
    "                for k,v in value.items():\n",
    "                    if k in dict_temp or k in all_keys: \n",
    "                        key_to_replace = k\n",
    "                        key_loc = locate_element(duplicate_remover_dict,key_to_replace)\n",
    "                        key_new = key_to_replace+'_'+str(key_loc[len(key_loc)-2])\n",
    "                        replacement_dict = duplicate_remover_dict\n",
    "                        for key in key_loc[:len(key_loc)-1]:\n",
    "                            replacement_dict = replacement_dict[key]\n",
    "                        replacement_dict[key_new] = replacement_dict.pop(key_to_replace)\n",
    "                        no_dupes = 1\n",
    "                    else:\n",
    "                        all_keys_temp.update({k:v})\n",
    "                if no_dupes ==1:\n",
    "                    all_keys_temp = {}\n",
    "                    break\n",
    "                else:\n",
    "                    dict_temp.update(value)\n",
    "            \n",
    "            elif type(value)==list and value != []:\n",
    "                if all(type(item) == dict for item in value):\n",
    "                    for item in value:\n",
    "                        for k,v in item.items():\n",
    "                            if k in all_keys or k in dict_temp:\n",
    "                                key_to_replace = k\n",
    "                                key_loc = locate_element(duplicate_remover_dict,key_to_replace)\n",
    "                                key_new = key_to_replace+'_'+str(key_loc[len(key_loc)-2])\n",
    "                                replacement_dict = duplicate_remover_dict\n",
    "                                for key in key_loc[:len(key_loc)-1]:\n",
    "                                    replacement_dict = replacement_dict[key]\n",
    "                                replacement_dict[key_new] = replacement_dict.pop(key_to_replace)\n",
    "                                no_dupes = 1\n",
    "                            else:\n",
    "                                all_keys_temp.update({k:v})\n",
    "                        if no_dupes ==1:\n",
    "                            all_keys_temp = {}\n",
    "                            break\n",
    "                        else:\n",
    "                            dict_temp.update(item)\n",
    "                else:\n",
    "                    all_keys_temp.update({key:value})\n",
    "                    for i in value:\n",
    "                        list_check.append(i)\n",
    "                    # Append any dict in value (list) to dict_temp.update \n",
    "                    while list_check != []:\n",
    "                        list_temp = []\n",
    "                        for item in list_check:\n",
    "                            if type(item)==dict:\n",
    "                                # Check if item not duplicate \n",
    "                                for k,v in item.items():\n",
    "                                    if k in all_keys or k in dict_temp:\n",
    "                                        key_to_replace = k\n",
    "                                        key_loc = locate_element(duplicate_remover_dict,key_to_replace)\n",
    "                                        key_new = key_to_replace+'_'+str(key_loc[len(key_loc)-2])\n",
    "                                        replacement_dict = duplicate_remover_dict\n",
    "                                        for key in key_loc[:len(key_loc)-1]:\n",
    "                                            replacement_dict = replacement_dict[key]\n",
    "                                        replacement_dict[key_new] = replacement_dict.pop(key_to_replace)\n",
    "                                        no_dupes = 1\n",
    "                                    else:\n",
    "                                        all_keys_temp.update({k:v})\n",
    "                                if no_dupes == 1:\n",
    "                                    all_keys_temp = {}\n",
    "                                    break\n",
    "                                else:\n",
    "                                    dict_temp.update(item)\n",
    "                            elif type(item)==list:\n",
    "                                for i in item:\n",
    "                                    list_temp.append(i)\n",
    "                            else:\n",
    "                                continue\n",
    "                        list_check = list_temp\n",
    "                              \n",
    "            else:\n",
    "                if key in all_keys or key in dict_temp: \n",
    "                    key_to_replace = key\n",
    "                    key_loc = locate_element(d_no_dupes,key_to_replace) #You use d_no_dupes because duplicate_remover dict has moved 1 level down i.e. key will not be in there anymore\n",
    "                    type_to_check = d_no_dupes\n",
    "                    for key in key_loc[:len(key_loc)-1]:\n",
    "                        type_to_check = type_to_check[key]\n",
    "                    if len(key_loc) == 1 or type(type_to_check) !=dict:\n",
    "                        key_new = key_to_replace+'_'+str(key_loc[len(key_loc)-2])\n",
    "                        replacement_dict = d_no_dupes\n",
    "                        #You use d_no_dupes because duplicate_remover dict has moved 1 level down i.e. key will not be in there anymore\n",
    "                        if len(key_loc)>1:\n",
    "                            for key in key_loc[:len(key_loc)-1]:\n",
    "                                replacement_dict = replacement_dict[key]\n",
    "                        replacement_dict[key_new] = replacement_dict.pop(key_to_replace)\n",
    "                        duplicate_remover_dict.pop(key)\n",
    "                        no_dupes = 1\n",
    "                        all_keys_temp = {}\n",
    "                        break\n",
    "                else:\n",
    "                    all_keys_temp.update({key:value})\n",
    "        if no_dupes ==0:\n",
    "            duplicate_remover_dict = dict_temp\n",
    "            all_keys.update(all_keys_temp)\n",
    "    return d_no_dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data dictionary\n",
    "key_list = [\n",
    "    'id',\n",
    "    'name',\n",
    "    'firstname',\n",
    "    'lastname',\n",
    "    'headline',\n",
    "    'subdomain', \n",
    "#    'name', need to rename using retrieve key function\n",
    "    'shortcode',\n",
    "    'title',\n",
    "    'stage',\n",
    "    'disqualified',\n",
    "    'disqualification_reason',\n",
    "    'hired_at',\n",
    "    'sourced',\n",
    "    'profile_url',\n",
    "    'address',\n",
    "    'phone',\n",
    "    'email',\n",
    "    'domain',\n",
    "    'created_at',\n",
    "    'updated_at',\n",
    "]\n",
    "# Create DataFrame column labels\n",
    "df_dict = {}\n",
    "for key in key_list:\n",
    "    df_dict[key]=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'candidates'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-17dd115a0368>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mr_last_cand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'limit='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'&created_after='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_last_cand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'candidates'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoday\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mr_last_cand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://jdriven.workable.com/spi/v3/candidates?limit=100&created_after='\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'candidates'"
     ]
    }
   ],
   "source": [
    "# Get latest entry:\n",
    "\n",
    "section = 'candidates?'\n",
    "limit='100'\n",
    "\n",
    "d = datetime.datetime.today().isoformat()\n",
    "r_last_cand = requests.get(url+section+'limit='+limit+'&created_after='+d+'.json', headers=headers)\n",
    "while len(r_last_cand.json()['candidates'])==0:\n",
    "    d = (datetime.datetime.today() - datetime.timedelta(days=1)).isoformat()\n",
    "    r_last_cand = requests.get('https://jdriven.workable.com/spi/v3/candidates?limit=100&created_after='+d+'.json', headers=headers)\n",
    "last_id = r_last_cand.json()['candidates'][-1]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-09-16T09:39:57.271293'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.today().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_last_cand = requests.get(url+section+'limit='+limit+'&created_after=2019-09-13T09:39:57.271293.json', headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_id = r_last_cand.json()['candidates'][-1]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last candidate entry retrieved\n"
     ]
    }
   ],
   "source": [
    "# Build initial database\n",
    "\n",
    "# Get first page\n",
    "cand_id_list = []\n",
    "workable_start_date = '2010-01-01T10:10:10Z'\n",
    "r_cand = requests.get(url+'candidates?limit='+limit+'&created_after='+workable_start_date+'.json', headers=headers)\n",
    "for cand in r_cand.json()['candidates']:\n",
    "    cand_id_list.append(cand['id'])\n",
    "    for k in key_list:\n",
    "        loc = locate_element(cand,k)\n",
    "        v = cand\n",
    "        for i in loc:\n",
    "            v = v[i]\n",
    "        df_dict[k].append(v)\n",
    "since_id=r_cand.json()['paging']['next'].split(\"since_id=\",1)[1]\n",
    "\n",
    "# Get next pages\n",
    "try:\n",
    "    while last_id not in cand_id_list:\n",
    "        r_cand = requests.get('https://jdriven.workable.com/spi/v3/candidates?limit=100&since_id='+since_id+'.json', headers=headers)\n",
    "        for cand in r_cand.json()['candidates']:\n",
    "            cand_id_list.append(cand['id'])\n",
    "            for k in key_list:\n",
    "                loc = locate_element(cand,k)\n",
    "                v = cand\n",
    "                for i in loc:\n",
    "                    v = v[i]\n",
    "                df_dict[k].append(v)\n",
    "        since_id=r_cand.json()['paging']['next'].split(\"since_id=\",1)[1]\n",
    "except KeyError:\n",
    "    print('Last candidate entry retrieved')\n",
    "    last_id = cand_id_list[-1]\n",
    "    last_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5045"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cand_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_cand_db(last_id, cand_id_list):\n",
    "    '''\n",
    "    Update candidate database with any new candidates added after last candidate entry in DB\n",
    "    Key Arguments:\n",
    "    last_id -- id of last entry of the candidate database\n",
    "    '''\n",
    "    #Retrieve latest candidates\n",
    "    r_cand_upd = requests.get(url+'candidates?limit='+limit+'&since_id='+last_id+'.json', headers=headers)\n",
    "    if len(r_cand_upd.json()['candidates'])>1: # first item in the candidates list is already in the DB\n",
    "        for new_cand in r_cand_upd.json()['candidates'][1:]:\n",
    "            cand_id_list.append(new_cand['id'])\n",
    "        last_id=new_cand['id']\n",
    "        return cand_id_list\n",
    "    else:\n",
    "        print('Nothing to add')\n",
    "        print(cand_id_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'update_cand_db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-796e19a8bb07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mupdate_cand_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcand_id_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'update_cand_db' is not defined"
     ]
    }
   ],
   "source": [
    "update_cand_db(last_id, cand_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4756569'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cand_id_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame.from_dict(df_dict, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>firstname</th>\n",
       "      <th>lastname</th>\n",
       "      <th>headline</th>\n",
       "      <th>subdomain</th>\n",
       "      <th>shortcode</th>\n",
       "      <th>title</th>\n",
       "      <th>stage</th>\n",
       "      <th>disqualified</th>\n",
       "      <th>disqualification_reason</th>\n",
       "      <th>hired_at</th>\n",
       "      <th>sourced</th>\n",
       "      <th>profile_url</th>\n",
       "      <th>address</th>\n",
       "      <th>phone</th>\n",
       "      <th>email</th>\n",
       "      <th>domain</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5140</td>\n",
       "      <td>4734f71</td>\n",
       "      <td>Dan Coman</td>\n",
       "      <td>Dan</td>\n",
       "      <td>Coman</td>\n",
       "      <td>None</td>\n",
       "      <td>jdriven</td>\n",
       "      <td>8176464B66</td>\n",
       "      <td>Front-End Developer (Divotion)</td>\n",
       "      <td>Review</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>https://jdriven.workable.com/backend/jobs/3403...</td>\n",
       "      <td>None</td>\n",
       "      <td>+31622090488</td>\n",
       "      <td>dancoman@live.com</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-09-11T14:54:28Z</td>\n",
       "      <td>2019-09-12T14:52:31Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5141</td>\n",
       "      <td>4742d0b</td>\n",
       "      <td>Max Ortega</td>\n",
       "      <td>Max</td>\n",
       "      <td>Ortega</td>\n",
       "      <td>None</td>\n",
       "      <td>jdriven</td>\n",
       "      <td>5E4DC1408A</td>\n",
       "      <td>Data Scientist (Vantage AI)</td>\n",
       "      <td>To schedule</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>https://jdriven.workable.com/backend/jobs/6865...</td>\n",
       "      <td>None</td>\n",
       "      <td>+31 653563356</td>\n",
       "      <td>maxdelvecchyo@gmail.com</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-09-12T08:37:57Z</td>\n",
       "      <td>2019-09-12T15:57:41Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5142</td>\n",
       "      <td>4743315</td>\n",
       "      <td>Razieh Taghavi</td>\n",
       "      <td>Razieh</td>\n",
       "      <td>Taghavi</td>\n",
       "      <td>None</td>\n",
       "      <td>jdriven</td>\n",
       "      <td>5E4DC1408A</td>\n",
       "      <td>Data Scientist (Vantage AI)</td>\n",
       "      <td>Review</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>https://jdriven.workable.com/backend/jobs/6865...</td>\n",
       "      <td>None</td>\n",
       "      <td>+31626663766</td>\n",
       "      <td>raztaghavi@gmail.com</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-09-11T06:52:19Z</td>\n",
       "      <td>2019-09-13T09:37:39Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5143</td>\n",
       "      <td>4747ac0</td>\n",
       "      <td>Sietse van der Bom</td>\n",
       "      <td>Sietse</td>\n",
       "      <td>van der Bom</td>\n",
       "      <td>None</td>\n",
       "      <td>jdriven</td>\n",
       "      <td>8176464B66</td>\n",
       "      <td>Front-End Developer (Divotion)</td>\n",
       "      <td>Review</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>https://jdriven.workable.com/backend/jobs/3403...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>parqbanq@gmail.com</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-09-12T14:22:50Z</td>\n",
       "      <td>2019-09-12T15:39:52Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5144</td>\n",
       "      <td>4756569</td>\n",
       "      <td>Casper Slenders</td>\n",
       "      <td>Casper</td>\n",
       "      <td>Slenders</td>\n",
       "      <td>None</td>\n",
       "      <td>jdriven</td>\n",
       "      <td>5E4DC1408A</td>\n",
       "      <td>Data Scientist (Vantage AI)</td>\n",
       "      <td>Review</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>https://jdriven.workable.com/backend/jobs/6865...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>casperslenders@live.nl</td>\n",
       "      <td>None</td>\n",
       "      <td>2019-09-13T10:43:20Z</td>\n",
       "      <td>2019-09-16T06:54:07Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                name firstname     lastname headline subdomain  \\\n",
       "5140  4734f71           Dan Coman       Dan        Coman     None   jdriven   \n",
       "5141  4742d0b          Max Ortega       Max       Ortega     None   jdriven   \n",
       "5142  4743315      Razieh Taghavi    Razieh      Taghavi     None   jdriven   \n",
       "5143  4747ac0  Sietse van der Bom    Sietse  van der Bom     None   jdriven   \n",
       "5144  4756569     Casper Slenders    Casper     Slenders     None   jdriven   \n",
       "\n",
       "       shortcode                           title        stage  disqualified  \\\n",
       "5140  8176464B66  Front-End Developer (Divotion)       Review         False   \n",
       "5141  5E4DC1408A     Data Scientist (Vantage AI)  To schedule         False   \n",
       "5142  5E4DC1408A     Data Scientist (Vantage AI)       Review         False   \n",
       "5143  8176464B66  Front-End Developer (Divotion)       Review         False   \n",
       "5144  5E4DC1408A     Data Scientist (Vantage AI)       Review         False   \n",
       "\n",
       "     disqualification_reason hired_at  sourced  \\\n",
       "5140                    None     None     True   \n",
       "5141                    None     None     True   \n",
       "5142                    None     None     True   \n",
       "5143                    None     None     True   \n",
       "5144                    None     None     True   \n",
       "\n",
       "                                            profile_url address  \\\n",
       "5140  https://jdriven.workable.com/backend/jobs/3403...    None   \n",
       "5141  https://jdriven.workable.com/backend/jobs/6865...    None   \n",
       "5142  https://jdriven.workable.com/backend/jobs/6865...    None   \n",
       "5143  https://jdriven.workable.com/backend/jobs/3403...    None   \n",
       "5144  https://jdriven.workable.com/backend/jobs/6865...    None   \n",
       "\n",
       "              phone                    email domain            created_at  \\\n",
       "5140   +31622090488        dancoman@live.com   None  2019-09-11T14:54:28Z   \n",
       "5141  +31 653563356  maxdelvecchyo@gmail.com   None  2019-09-12T08:37:57Z   \n",
       "5142   +31626663766     raztaghavi@gmail.com   None  2019-09-11T06:52:19Z   \n",
       "5143           None       parqbanq@gmail.com   None  2019-09-12T14:22:50Z   \n",
       "5144           None   casperslenders@live.nl   None  2019-09-13T10:43:20Z   \n",
       "\n",
       "                updated_at  \n",
       "5140  2019-09-12T14:52:31Z  \n",
       "5141  2019-09-12T15:57:41Z  \n",
       "5142  2019-09-13T09:37:39Z  \n",
       "5143  2019-09-12T15:39:52Z  \n",
       "5144  2019-09-16T06:54:07Z  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['id object',\n",
    " 'name object',\n",
    " 'firstname object',\n",
    " 'lastname object',\n",
    " 'headline object',\n",
    " 'subdomain object',\n",
    " 'shortcode object',\n",
    " 'title object',\n",
    " 'stage object',\n",
    " 'disqualified bool',\n",
    " 'disqualification_reason object',\n",
    " 'hired_at object',\n",
    " 'sourced bool',\n",
    " 'profile_url object',\n",
    " 'address object',\n",
    " 'phone object',\n",
    " 'email object',\n",
    " 'domain object',\n",
    " 'created_at object',\n",
    " 'updated_at object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id TEXT,name TEXT,firstname TEXT,lastname TEXT,headline TEXT,subdomain TEXT,shortcode TEXT,title TEXT,stage TEXT,disqualified bool,disqualification_reason TEXT,hired_at TEXT,sourced bool,profile_url TEXT,address TEXT,phone TEXT,email TEXT,domain TEXT,created_at TEXT,updated_at TEXT'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create sql_create_table_query = ... columns and data type\n",
    "col_dtype = []\n",
    "for col in df.columns.tolist():\n",
    "    col_dtype.append(col+' '+str(df[col].dtype))\n",
    "#Replace objects with TEXT as object is not a data type that can be parsed into CREATE TABLE statement\n",
    "col_dtype = [s.replace('object', 'TEXT') for s in col_dtype]\n",
    "table_col = ','.join(col_dtype)\n",
    "table_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL DB connection is closed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn = mysql.connector.connect(user='root', password='maartens1991',host='localhost', database='recruitment_dashboard')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create table REPLACE WITH CODE BELOW TO MAKE QUERY STATEMENT AND THEN EXECUTE\n",
    "    # Create sql_create_table_query = ... columns and data type\n",
    "    col_dtype = []\n",
    "    for col in df.columns.tolist():\n",
    "        col_dtype.append(col+' '+str(df[col].dtype))\n",
    "    #Replace objects with TEXT as object is not a data type that can be parsed into CREATE TABLE statement\n",
    "    col_dtype = [s.replace('object', 'TEXT') for s in col_dtype]\n",
    "    table_col = ','.join(col_dtype)\n",
    "    \n",
    "    sql_create_table_query = \"\"\"CREATE TABLE candidates (%s)\"\"\"%(table_col)\n",
    "    cursor.execute(sql_create_table_query)\n",
    "    \n",
    "    # Save (commit) the changes\n",
    "    conn.commit()\n",
    "\n",
    "    # Select column names in order from 1st to last column\n",
    "    sql_select_query = '''SELECT column_name\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'recruitment_dashboard'\n",
    "    AND table_name   = 'candidates'\n",
    "    ORDER BY ORDINAL_POSITION\n",
    "    '''\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql_select_query)\n",
    "    records = cursor.fetchall() \n",
    "    columns = []\n",
    "    for i in records:\n",
    "        columns.append(i[0])\n",
    "    col_n = ','.join(columns)\n",
    "\n",
    "    #Records to insert in SQL table\n",
    "    records_to_insert = list(df.itertuples(index=False, name=None))\n",
    "    params = ['%s' for item in records_to_insert[0]] # always use '%s' no matter the data type of the column\n",
    "    var_string = ','.join(params)\n",
    "\n",
    "    #Insert all records into table\n",
    "    sql_insert_query = \"\"\"INSERT INTO candidates (%s) VALUES (%s);\"\"\" %(col_n,var_string)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.executemany(sql_insert_query, records_to_insert)\n",
    "    \n",
    "    #Commit all changes\n",
    "    conn.commit()\n",
    "\n",
    "except (Exception, mysql.connector.Error) as error:\n",
    "    print (\"Error while connecting to SQL DB\", error)\n",
    "finally:\n",
    "    #Closing database connection.\n",
    "    if(conn):\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"SQL DB connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5145"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(records_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'test.csv', header = df.columns.tolist(), encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'name',\n",
       " 'firstname',\n",
       " 'lastname',\n",
       " 'headline',\n",
       " 'subdomain',\n",
       " 'shortcode',\n",
       " 'title',\n",
       " 'stage',\n",
       " 'disqualified',\n",
       " 'disqualification_reason',\n",
       " 'hired_at',\n",
       " 'sourced',\n",
       " 'profile_url',\n",
       " 'address',\n",
       " 'phone',\n",
       " 'email',\n",
       " 'domain',\n",
       " 'created_at',\n",
       " 'updated_at']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add DF to SQL using df.to_sql()\n",
    "#https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.fillna(value='unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv(r'test.csv', header = df.columns.tolist(), encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://scottsfarley.com/research/cloudcomputing/2016/06/02/adventures-in-google-cloud-I.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud SQL Connection\n",
    "conn = mysql.connector.connect(user='root', password='MjB6KtDfI4pkzKr9',host='34.90.224.97', database='recruitment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mysql.connector.connection_cext.CMySQLConnection at 0x19350439a48>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()\n",
    "cursor.execute('''CREATE TABLE test_table (date text, trans text, symbol text, qty real, price real)''')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
