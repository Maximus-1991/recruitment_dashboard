{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "#How to safely pass in PW (where to store it?)\n",
    "#Check if can find source in API\n",
    "#Update DB function: access db, get last entry, then use since_id\n",
    "#Create functions\n",
    "#Create docker image\n",
    "#Host image in cloud\n",
    "#Flask webapp\n",
    "\n",
    "# Check why max_id doesnt work\n",
    "\n",
    "# Write .py functions\n",
    "# Dockers\n",
    "\n",
    "# Retrieve all keys in dictionary that do not contain\n",
    "\n",
    "# need to make pipeline to transform all entries in DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import pytz\n",
    "import random\n",
    "from datetime import date\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Sequence\n",
    "from itertools import chain, count, tee\n",
    "\n",
    "headers={'Authorization': 'Bearer 229aa3876e4fc4447460a13da7f57d1be4111202e1d56d4d0231fb932c1e7cd1'}\n",
    "url = 'https://jdriven.workable.com/spi/v3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_element(data,look_up_elem):\n",
    "    '''\n",
    "    Function to locate the exact location of a an element in a data structure\n",
    "    '''\n",
    "    data_orig = data\n",
    "    loc_list = []\n",
    "    \n",
    "    #### Step 1: Create loop: while look_up_elem not in loc_list\n",
    "    while look_up_elem not in loc_list:\n",
    "\n",
    "        data = data_orig\n",
    "        if loc_list != []:\n",
    "            for location in loc_list:\n",
    "                data = data[location]     \n",
    "        \n",
    "        #### Step 2: Create loop for each element in data. This element needs to be appended to loc_list if element is found in (sub-levels of) this element\n",
    "        # Combine step 4 and 5 in one function. Function is to flatten the data and check if look_up_elem is present in data. If element is found, return loc_list\n",
    "        def check_branche(data):\n",
    "                    #### Step 2: Check if look_up_element is present on 1st level of data\n",
    "            if look_up_elem in data:\n",
    "                loc_list.append(look_up_elem)\n",
    "                return loc_list\n",
    "\n",
    "            #### Step 3: If element not present on 1st level, filter out strings and integers from data. Method is different for different data types\n",
    "            # Note: data_tuple = () (will be problematic, as you cannot append elements to a tuple). We may be able to add items from tuple to list as tuple is also a Sequence\n",
    "\n",
    "            # Define data_elements\n",
    "            if type(data)==dict: \n",
    "                data_elements = list(data.keys())\n",
    "            elif type(data)==list: \n",
    "                data_elements = list(range(len(data)))\n",
    "            # elif type(data)==tuple: \n",
    "                #data_elements = list(range(len(data)))\n",
    "\n",
    "            else:\n",
    "                return \"Element not present\"\n",
    "                      \n",
    "            for element in data_elements:\n",
    "\n",
    "                data_to_check = data[element]\n",
    "\n",
    "                # Define data_dict, data_list and data_tuple\n",
    "                if type(data_to_check)==dict:\n",
    "                    data_dict = data_to_check\n",
    "                    data_list = []\n",
    "                    data_tuple = ()\n",
    "                elif type(data_to_check)==list:\n",
    "                    data_dict = {}\n",
    "                    data_list = data_to_check\n",
    "                    data_tuple = ()\n",
    "                elif type(data_to_check)==tuple:\n",
    "                    data_dict = {}\n",
    "                    data_list = []\n",
    "                    data_tuple = data_to_check\n",
    "                elif type(data_to_check)!=dict and type(data_to_check)!=list and type(data_to_check)!=tuple:\n",
    "                    continue\n",
    "                else:\n",
    "                    return \"Error\"\n",
    "\n",
    "                #### Step 5: Enter while loop (is within the for loop of step 4). From the filtered data obtained in step 3, divide the different elements into its data type. Then, flatten type(data) data type first and then the other two data types\n",
    "                # When look_up_elem is found, append element to loc_list and return loc_list\n",
    "                while data_dict != {} or data_list != [] or data_tuple !=():\n",
    "                    # Flatten dictionary and check if element is present on any of the levels and add list elements to data_list\n",
    "                    # After first round, if any elements were added to data_dict, go through these added elements\n",
    "                    while data_dict != {}:\n",
    "                        if look_up_elem in data_dict:\n",
    "                            loc_list.append(element)\n",
    "                            return \n",
    "                            \n",
    "                        data_dict_temp = {}\n",
    "                        # Filter the elements in data_dict\n",
    "                        for key,value in iter(data_dict.items()):\n",
    "                            if type(value)==dict:\n",
    "                                data_dict_temp.update(value)\n",
    "                            elif type(value)==list:\n",
    "                                data_list.append(value)\n",
    "                            #elif type(value)==tuple:\n",
    "                            #    test_tuple\n",
    "                            # to check if tuple is also a sequence, can also use chain(element) for this\n",
    "                            else:\n",
    "                                \"Element is string or integer\"\n",
    "                        data_dict = data_dict_temp\n",
    "\n",
    "                    # After data_dict is (temporarily) exhausted, go through data_list\n",
    "                    while data_list != []:\n",
    "                        if look_up_elem in data_list:\n",
    "                            loc_list.append(element)\n",
    "                            return loc_list\n",
    "\n",
    "                        data_list_temp = []\n",
    "                        # Filter the elements in data_dict\n",
    "                        for item in data_list:\n",
    "                            if type(item)==dict:\n",
    "                                data_dict.update(item)\n",
    "                            elif type(item)==list:\n",
    "                                for i in item:\n",
    "                                    data_list_temp.append(i)\n",
    "                            else:\n",
    "                                \"Element is string or integer\"\n",
    "                        data_list = data_list_temp\n",
    "\n",
    "                    # After data_list is (temporarily) exhausted, go through data_tuple\n",
    "                    #while data_tuple !=():\n",
    "                        # Flatten tuple, check if element is present on any of the levels and add dictionary and list elements to data_dict or data_list\n",
    "                        #pass\n",
    "\n",
    "            if look_up_elem not in loc_list:\n",
    "                return \"Element not Found\"\n",
    "        check_branche(data)\n",
    "    \n",
    "    return loc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_api_entry(url=url,headers=headers):\n",
    "    '''\n",
    "    Function to retrieve the last entry in Workable through API\n",
    "    \n",
    "    Inputs:\n",
    "    url: url of the Workable API\n",
    "    headers: headers to connect to the API\n",
    "    \n",
    "    Outputs:\n",
    "    'id' of the last entry\n",
    "    '''\n",
    "    section = 'candidates?'\n",
    "    limit='100'\n",
    "    d = datetime.datetime.today()\n",
    "    r_last_entry = requests.get(url+section+'limit='+limit+'&created_after='+d.isoformat()+'.json', headers=headers)\n",
    "    while len(r_last_entry.json()['candidates'])==0:\n",
    "        d = (d - datetime.timedelta(days=1))\n",
    "        r_last_entry = requests.get(url+section+'limit='+limit+'&created_after='+d.isoformat()+'.json', headers=headers)\n",
    "        time.sleep(0.9)\n",
    "    last_id = r_last_entry.json()['candidates'][-1]['id']\n",
    "    return last_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_api_id = last_api_entry(url,headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'48a4545'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_api_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_last_db_entry(pw,db_name=\"candidates5\",user='root',host='127.0.0.1',port='',database='recruitment_dashboard'):\n",
    "    '''\n",
    "    Function to retrieve candidate id of the last entry in the MySQL DB\n",
    "    Inputs:\n",
    "    db_name: Name of the to be created MySQL DB (default is 'candidates2')\n",
    "    user: user name of db (default is 'root')\n",
    "    pw: password of database\n",
    "    host = '127.0.0.1' if connecting to local DB\n",
    "    host = '<IP address of DB>' if connecting to a DB hosted externally\n",
    "    port: port (default is no port specified). Format is: ':<port>'\n",
    "    database: database schema name\n",
    "    \n",
    "    Outputs:\n",
    "    last_entry_id: 'id' of last entry in the MySQL DB\n",
    "    '''\n",
    "    conn = mysql.connector.connect(user=user, password=pw,host=host, database=database)\n",
    "    cursor = conn.cursor()\n",
    "    sql_select_query = \"\"\"SELECT id FROM %s ORDER BY created_at DESC LIMIT 1\"\"\"%(db_name)\n",
    "    cursor.execute(sql_select_query)\n",
    "    last_entry_id = cursor.fetchall() \n",
    "    last_entry_id = last_entry_id[0][0]\n",
    "    conn.close()\n",
    "    return last_entry_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_db_id = retrieve_last_db_entry(pw='maartens1991',db_name=\"candidates5\",user='root',host='127.0.0.1',port='',database='recruitment_dashboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ebb4b0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_db_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(workable_start_date='2010-01-01T10:10:10Z'):\n",
    "#    retrieve_page()\n",
    "#    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = [\n",
    "        'id',\n",
    "        'name',\n",
    "        'firstname',\n",
    "        'lastname',\n",
    "        'headline',\n",
    "        'subdomain', \n",
    "        'shortcode',\n",
    "        'title',\n",
    "        'stage',\n",
    "        'disqualified',\n",
    "        'disqualification_reason',\n",
    "        'hired_at',\n",
    "        'sourced',\n",
    "        'profile_url',\n",
    "        'address',\n",
    "        'phone',\n",
    "        'email',\n",
    "        'domain',\n",
    "        'created_at',\n",
    "        'updated_at',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db():\n",
    "    # Create empty df_dict\n",
    "    df_dict={}\n",
    "    for key in key_list:\n",
    "        df_dict[key]=[]\n",
    "    last_db_id = retrieve_last_db_entry(pw='maartens1991',db_name=\"candidates5\",user='root',host='127.0.0.1',port='',database='recruitment_dashboard')\n",
    "    df_dict, since_id, cand_id_list=get_cand_data(df_dict, url=url,headers=headers, limit='100', cand_id_list=[], start_id=last_db_id, start_date='', key_list=key_list)\n",
    "    while since_id!=None:\n",
    "        #get more pages\n",
    "    cand_id_list.remove(last_db_id)\n",
    "    if cand_id_list!=[]:\n",
    "        df_dict_cand=retrieve_activities(url,headers,cand_id_list)\n",
    "        df_cand=create_df(df_dict)\n",
    "        df_act=create_df(df_dict_cand)\n",
    "        df_comb=merge_df(df_cand,df_act,how='left', on=['id'])\n",
    "        df_comb=transform_df(df)\n",
    "        df_to_db(df_comb,pw,db_name='candidates5',user='root:',host='localhost',port='',schema='recruitment_dashboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty df_dict\n",
    "df_dict={}\n",
    "for key in key_list:\n",
    "    df_dict[key]=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cand_data(df_dict, url=url,headers=headers, limit='100', cand_id_list=[], start_id='', start_date='', key_list=key_list):  #request\n",
    "    if start_date != '':\n",
    "        created_after = '&created_after='+start_date\n",
    "    else:\n",
    "        created_after = ''\n",
    "    \n",
    "    if start_id != '':\n",
    "        start_id = '&since_id='+start_id\n",
    "    else:\n",
    "        start_id = ''\n",
    "    section = 'candidates?'\n",
    "    request = requests.get(url+section+'limit='+limit+created_after+start_id+'.json', headers=headers)  \n",
    "    for cand in request.json()['candidates']:\n",
    "        cand_id_list = cand_id_list\n",
    "        cand_id_list.append(cand['id'])\n",
    "        for k in key_list:\n",
    "            loc = locate_element(cand,k)\n",
    "            v = cand\n",
    "            for i in loc:\n",
    "                v = v[i]\n",
    "            df_dict[k].append(v)\n",
    "    try:\n",
    "        since_id=r_cand.json()['paging']['next'].split(\"since_id=\",1)[1]\n",
    "        return df_dict, since_id, cand_id_list\n",
    "    except:\n",
    "        since_id=None\n",
    "        return df_dict, since_id, cand_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': ['48a4545', '48a4545'],\n",
       "  'name': ['Jens Dudink', 'Jens Dudink'],\n",
       "  'firstname': ['Jens', 'Jens'],\n",
       "  'lastname': ['Dudink', 'Dudink'],\n",
       "  'headline': [None, None],\n",
       "  'subdomain': ['jdriven', 'jdriven'],\n",
       "  'shortcode': ['5E4DC1408A', '5E4DC1408A'],\n",
       "  'title': ['Data Scientist (Vantage AI)', 'Data Scientist (Vantage AI)'],\n",
       "  'stage': ['Review', 'Review'],\n",
       "  'disqualified': [False, False],\n",
       "  'disqualification_reason': [None, None],\n",
       "  'hired_at': [None, None],\n",
       "  'sourced': [True, True],\n",
       "  'profile_url': ['https://jdriven.workable.com/backend/jobs/686542/candidates/76169331',\n",
       "   'https://jdriven.workable.com/backend/jobs/686542/candidates/76169331'],\n",
       "  'address': [None, None],\n",
       "  'phone': ['(+31) 6 2446 4393', '(+31) 6 2446 4393'],\n",
       "  'email': ['ja.dudink@gmail.com', 'ja.dudink@gmail.com'],\n",
       "  'domain': [None, None],\n",
       "  'created_at': ['2019-10-04T12:32:48Z', '2019-10-04T12:32:48Z'],\n",
       "  'updated_at': ['2019-10-04T13:16:39Z', '2019-10-04T13:16:39Z']},\n",
       " None,\n",
       " ['48a4545'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cand_data(df_dict=df_dict, url=url,headers=headers, limit='100', cand_id_list=[], start_id=last_api_id, start_date='', key_list=key_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_all_pages(url,headers,df_dict,since_id,cand_id_list,last_id):\n",
    "    '''\n",
    "    Function to get retrieve general candidate data\n",
    "    Inputs:\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    try:\n",
    "        while last_id not in cand_id_list:\n",
    "            r_cand = requests.get(url+section+'limit='+limit+'&since_id='+since_id+'.json', headers=headers)\n",
    "            df_dict, since_id, cand_id_list\n",
    "            get_cand_data(df_dict, url=url,headers=headers, limit='100', cand_id_list=[], start_id='', start_date='', key_list=key_list)\n",
    "            time.sleep(0.9)\n",
    "        return df_dict, cand_id_list\n",
    "    except:\n",
    "        return df_dict, cand_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_activities(url,headers,cand_id_list):\n",
    "    '''\n",
    "    Function to create candidate activity dictionary\n",
    "    Inputs:\n",
    "    \n",
    "    df_dict: dictionary containing candidate data\n",
    "    Outputs:\n",
    "    \n",
    "    DataFrame containing the same candidate data as the input\n",
    "    '''\n",
    "    # Create DataFrame column labels\n",
    "    df_dict_cand = {}\n",
    "    key_list_cand = ['id','tags']\n",
    "    stage_name_list = [\n",
    "        'Sourced',\n",
    "        'Applied',\n",
    "        'Shortlisted',\n",
    "        'Talentpool',\n",
    "        'Review',\n",
    "        'To schedule',\n",
    "        'Inplannen 1e gesorek', #not in use anymore --> combine with 'To Schedule' --> delete\n",
    "        'Inplannen 1e gesprek', #not in use anymore --> combine with 'To Schedule' --> delete\n",
    "        'inplannen 2e gesprek', #not in use anymore --> combine with '1st Interview' --> delete\n",
    "        '1st Interview',\n",
    "        '1e gesprek', #not in use anymore --> combine with '1st Interview' --> delete\n",
    "        'Interview 1', #not in use anymore --> combine with '1st Interview' --> delete\n",
    "        '2nd Interview', \n",
    "        'Interview 2', #not in use anymore --> combine with '2nd Interview' --> delete\n",
    "        'Assessment', #not in use anymore --> combine with '2nd Interview' --> delete\n",
    "        '2e gesprek', #not in use anymore --> combine with '2nd Interview' --> delete\n",
    "        'Offer',\n",
    "        'Aanbieding', #not in use anymore --> combine with 'Offer' --> delete\n",
    "        'Hired',\n",
    "        'Aangenomen', #not in use anymore --> combine with 'Hired' --> delete\n",
    "        'Test Fase', #not in use anymore --> delete\n",
    "        'intern evalueren', #not in use anymore --> delete\n",
    "        'Plan 1', #not in use anymore --> delete\n",
    "        'Plan 2', #not in use anymore --> delete\n",
    "        'Vergaarbak' #not in use anymore --> delete\n",
    "    ]\n",
    "\n",
    "    #Add labels to dictionary\n",
    "    for key in key_list_cand:\n",
    "        df_dict_cand[key]=[]    \n",
    "    for key in stage_name_list:\n",
    "        df_dict_cand[key]=[]\n",
    "    df_dict_cand['disqualified_at']=[]\n",
    "\n",
    "    #Retrieve data through API\n",
    "    section = 'candidates/'\n",
    "    cnt = 0\n",
    "    other_st = {'st':[]} # Can be removed later, if appears all stages are listed in stage_name_list\n",
    "\n",
    "    for cand_id in cand_id_list[:50]: #[:50]  \n",
    "        print(cand_id)\n",
    "        r_cand_id = requests.get(url+section+cand_id+'.json', headers=headers)\n",
    "        time.sleep(1.0)\n",
    "        print('calls remaining:')\n",
    "        print(r_cand_id.headers['x-rate-limit-remaining'])\n",
    "        for k in key_list_cand:\n",
    "            loc = locate_element(r_cand_id.json()['candidate'],k)\n",
    "            v = r_cand_id.json()['candidate']\n",
    "            for i in loc:\n",
    "                v = v[i]\n",
    "            df_dict_cand[k].append(v)\n",
    "\n",
    "        # loop through activities for candidate cand_id\n",
    "        r_cand_id_act = requests.get(url+section+cand_id+'/activities'+'.json', headers=headers)\n",
    "        print('calls remaining:')\n",
    "        print(r_cand_id_act.headers['x-rate-limit-remaining'])\n",
    "        r_cand_id_act=r_cand_id_act.json()['activities']\n",
    "        time.sleep(1.0)\n",
    "        stages = deepcopy(stage_name_list)\n",
    "        disqualified=False\n",
    "        for act in r_cand_id_act:\n",
    "            if act['action']=='disqualified' and disqualified==False:\n",
    "                df_dict_cand['disqualified_at'].append(act['created_at'])\n",
    "                disqualified=True\n",
    "            if act['stage_name'] in stage_name_list:\n",
    "                if act['stage_name'] not in stages:\n",
    "                    continue\n",
    "                else:\n",
    "                    df_dict_cand[act['stage_name']].append(act['created_at'])\n",
    "                    stages.remove(act['stage_name'])\n",
    "            else:\n",
    "                if act['stage_name']!=None:\n",
    "                    print(act['stage_name'])\n",
    "                other_st['st'].append(act['stage_name'])\n",
    "        if disqualified==False:\n",
    "            df_dict_cand['disqualified_at'].append(np.nan)\n",
    "        for remaining_stage in stages:\n",
    "            df_dict_cand[remaining_stage].append(np.nan)         \n",
    "        cnt+=1\n",
    "        print(cnt)\n",
    "        time.sleep(0.5)\n",
    "    return df_dict_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict, cand_id_list = retrieve_all_pages(url,headers,df_dict,since_id,cand_id_list,last_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(df_dict):\n",
    "    '''\n",
    "    Convert dictionary into pandas DataFrame\n",
    "    Inputs:\n",
    "    \n",
    "    df_dict: dictionary containing candidate data\n",
    "    Outputs:\n",
    "    \n",
    "    DataFrame containing the same candidate data as the input\n",
    "    '''\n",
    "    df = pd.DataFrame.from_dict(df_dict, orient='columns')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_df(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_cand=retrieve_activities(url,headers,cand_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cand=create_df(df_dict_cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(df1,df2,how='left', on=['id']):\n",
    "    '''\n",
    "    Function to create candidate activity dictionary\n",
    "    Inputs:\n",
    "    \n",
    "    df_dict: dictionary containing candidate data\n",
    "    Outputs:\n",
    "    \n",
    "    DataFrame containing the same candidate data as the input\n",
    "    '''    \n",
    "    df = pd.merge(df1, df2, how=how, on=on)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = merge_df(df,df_cand,how='left', on=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df):\n",
    "    '''\n",
    "    Function to transform DataFrame\n",
    "    Inputs:\n",
    "    \n",
    "    df_dict: dictionary containing candidate data\n",
    "    Outputs:\n",
    "    \n",
    "    '''\n",
    "    #Rename duplicate column name to prevent error when creating SQL database\n",
    "    df=df.rename(columns = {'sourced':'is_sourced'})\n",
    "    \n",
    "    #Replace and Delete columns\n",
    "    df['To schedule'][(df['Inplannen 1e gesorek'].isnull()==False) & (df['To schedule'].isnull()==True)]=df['Inplannen 1e gesorek'][(df['Inplannen 1e gesorek'].isnull()==False) & (df['To schedule'].isnull()==True)]\n",
    "    df.drop('Inplannen 1e gesorek',axis=1,inplace = True)\n",
    "    df['To schedule'][(df['Inplannen 1e gesprek'].isnull()==False) & (df['To schedule'].isnull()==True)]=df['Inplannen 1e gesprek'][(df['Inplannen 1e gesprek'].isnull()==False) & (df['To schedule'].isnull()==True)]\n",
    "    df.drop('Inplannen 1e gesprek',axis=1,inplace = True)\n",
    "    df['1st Interview'][(df['inplannen 2e gesprek'].isnull()==False) & (df['1st Interview'].isnull()==True)]=df['inplannen 2e gesprek'][(df['inplannen 2e gesprek'].isnull()==False) & (df['1st Interview'].isnull()==True)]\n",
    "    df.drop('inplannen 2e gesprek',axis=1,inplace = True)\n",
    "    df['1st Interview'][(df['1e gesprek'].isnull()==False) & (df['1st Interview'].isnull()==True)]=df['1e gesprek'][(df['1e gesprek'].isnull()==False) & (df['1st Interview'].isnull()==True)]\n",
    "    df.drop('1e gesprek',axis=1,inplace = True)\n",
    "    df['1st Interview'][(df['Interview 1'].isnull()==False) & (df['1st Interview'].isnull()==True)]=df['Interview 1'][(df['Interview 1'].isnull()==False) & (df['1st Interview'].isnull()==True)]\n",
    "    df.drop('Interview 1',axis=1,inplace = True)\n",
    "    df['2nd Interview'][(df['Interview 2'].isnull()==False) & (df['2nd Interview'].isnull()==True)]=df['Interview 2'][(df['Interview 2'].isnull()==False) & (df['2nd Interview'].isnull()==True)]\n",
    "    df.drop('Interview 2',axis=1,inplace = True)\n",
    "    df['2nd Interview'][(df['Assessment'].isnull()==False) & (df['2nd Interview'].isnull()==True)]=df['Assessment'][(df['Assessment'].isnull()==False) & (df['2nd Interview'].isnull()==True)]\n",
    "    df.drop('Assessment',axis=1,inplace = True)\n",
    "    df['2nd Interview'][(df['2e gesprek'].isnull()==False) & (df['2nd Interview'].isnull()==True)]=df['2e gesprek'][(df['2e gesprek'].isnull()==False) & (df['2nd Interview'].isnull()==True)]\n",
    "    df.drop('2e gesprek',axis=1,inplace = True)\n",
    "    df['Offer'][(df['Aanbieding'].isnull()==False) & (df['Offer'].isnull()==True)]=df['Aanbieding'][(df['Aanbieding'].isnull()==False) & (df['Offer'].isnull()==True)]\n",
    "    df.drop('Aanbieding',axis=1,inplace = True)\n",
    "\n",
    "    df['Hired'][(df['Aangenomen'].isnull()==False) & (df['Hired'].isnull()==True)]=df['Aangenomen'][(df['Aangenomen'].isnull()==False) & (df['Hired'].isnull()==True)]\n",
    "    df.drop('Aangenomen',axis=1,inplace = True)\n",
    "\n",
    "    # Delete Columns\n",
    "    df.drop('Test Fase',axis=1,inplace = True)\n",
    "    df.drop('intern evalueren',axis=1,inplace = True)\n",
    "    df.drop('Plan 1',axis=1,inplace = True)\n",
    "    df.drop('Plan 2',axis=1,inplace = True)\n",
    "    df.drop('Vergaarbak',axis=1,inplace = True)\n",
    "\n",
    "    #Remove only for now (will be used for source of candidate)\n",
    "    df.drop('tags',axis=1,inplace = True)\n",
    "\n",
    "    #Replace np.nan with None, as None is accepted if df is written to a DB using df.to_sql\n",
    "    #Note that None will only be converted to NULL in SQL if df.to_sql is used, not using executemany\n",
    "    #NaT is converted as None if using to_sql\n",
    "    df = df.where((pd.notnull(df)), None)\n",
    "\n",
    "    #Convert None to 'nan' if getting errors when inserting into MySQL DB\n",
    "    #df.fillna(value='nan', inplace=True)\n",
    "\n",
    "    #Convert date columns into DATE columns with specified format\n",
    "    date_cols = [\n",
    "            'hired_at',\n",
    "            'Sourced',\n",
    "            'Applied',\n",
    "            'Shortlisted',\n",
    "            'Talentpool',\n",
    "            'Review',\n",
    "            'To schedule',\n",
    "            '1st Interview',\n",
    "            '2nd Interview',\n",
    "            'Offer',\n",
    "            'Hired',\n",
    "            'disqualified_at'\n",
    "    ]\n",
    "    for date_col in date_cols:\n",
    "        #df[date_col]=pd.to_datetime(df[date_col])\n",
    "        df[date_col]=pd.to_datetime(pd.to_datetime(df[date_col]).dt.strftime('%Y-%m-%d'))\n",
    "    # Make pipeline for this, using typeselector\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col]=df[col].str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "            df[col][df[col].isnull()==False]=df[col][df[col].isnull()==False].apply(lambda x: x.lower())    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = transform_df(df_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_db(df,pw,db_name='candidates5',user='root:',host='localhost',port='',schema='recruitment_dashboard'):\n",
    "    '''\n",
    "    Function to write DataFrame to MySQL DB\n",
    "    Inputs:\n",
    "    df: DataFrame containing all candidate data\n",
    "    db_name: Name of the to be created MySQL DB (default is 'candidates2')\n",
    "    user: user name of db (default is 'root')\n",
    "    pw: password of database\n",
    "    host = 'localhost' if connecting to local DB\n",
    "    host = '<IP address of DB>' if connecting to a DB hosted externally\n",
    "    port: port (default is no port specified). Format is: ':<port>'\n",
    "    schema: database schema name\n",
    "    \n",
    "    Outputs:\n",
    "    Populated MySQL Database\n",
    "    '''\n",
    "    engine = create_engine('mysql+pymysql://'+user+pw+'@'+host+port+'/'+schema, echo = False)\n",
    "    df.to_sql(name = db_name, con = engine, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_db(df_comb,db_name='candidates5',user='root:',pw='maartens1991',host='localhost',port='',schema='recruitment_dashboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to Google Cloud DB\n",
    "engine_cloud = create_engine('mysql+pymysql://root:MjB6KtDfI4pkzKr9@34.90.224.97/recruitment', echo = False)\n",
    "df_comb.to_sql(name = 'candidates2', con = engine_cloud, if_exists = 'append', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def db_to_df(pw,db_name=\"candidates5\",user='root',host='127.0.0.1',port='',database='recruitment_dashboard'):\n",
    "    '''\n",
    "    Function to convert MySQL DB to DataFrame\n",
    "    Inputs:\n",
    "    db_name: Name of the to be created MySQL DB (default is 'candidates2')\n",
    "    user: user name of db (default is 'root')\n",
    "    pw: password of database\n",
    "    host = '127.0.0.1' if connecting to local DB\n",
    "    host = '<IP address of DB>' if connecting to a DB hosted externally\n",
    "    port: port (default is no port specified). Format is: ':<port>'\n",
    "    database: database schema name\n",
    "    \n",
    "    Outputs:\n",
    "    Populated MySQL Database\n",
    "    '''\n",
    "    conn = mysql.connector.connect(user=user, password=pw,host=host, database=database)\n",
    "    df = pd.read_sql(\"SELECT * from \"+db_name, conn)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_db = db_to_df(pw='maartens1991',db_name=\"candidates5\",user='root',host='127.0.0.1',port='',database='recruitment_dashboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_funnel(df,start_year=2014, start_month=1, start_day=1, end_year=2017, end_month=1, end_day=1):\n",
    "    '''\n",
    "    Function to retrieve interview process funnel for specified time frame\n",
    "    Inputs:\n",
    "    df: DataFrame containing info about all candidates\n",
    "    start_date\n",
    "    end_date\n",
    "\n",
    "    \n",
    "    Outputs:\n",
    "    funnel_dict: dictionary containing the number of candidates in each funnel stage for specified time frame\n",
    "    '''\n",
    "    start_date = datetime.datetime(start_year, start_month, start_date)\n",
    "    end_date = datetime.datetime(end_year, end_month, end_date)\n",
    "\n",
    "    funnel_stages = [\n",
    "            'Sourced',\n",
    "            'Applied',\n",
    "            'Shortlisted',\n",
    "            'Talentpool',\n",
    "            'Review',\n",
    "            'To schedule',\n",
    "            '1st Interview',\n",
    "            '2nd Interview',\n",
    "            'Offer',\n",
    "            'Hired',\n",
    "            'disqualified_at'\n",
    "    ]\n",
    "    funnel_dict={}\n",
    "    for funnel in funnel_stages:\n",
    "        funnel_dict[funnel]=df[funnel][(df[funnel]>=start_date) & (df[funnel]<=end_date)].count()\n",
    "    return funnel_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_funnel(df,start_year=2014, start_month=1, start_day=1, end_year=2017, end_month=1, end_day=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure(figsize=(20,6)) # figsize=(width,height)\n",
    "plt.figure(figsize=(12, 12), dpi=80)\n",
    "plt.bar(range(len(funnel_dict)), list(funnel_dict.values()), align='center')\n",
    "plt.xticks(range(len(funnel_dict)), list(funnel_dict.keys()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_last_db_entry(pw='maartens1991',db_name=\"candidates5\",user='root',host='127.0.0.1',port='',database='recruitment_dashboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db(url,headers,pw,db_name=\"candidates5\",user='root',host='127.0.0.1',port='',database='recruitment_dashboard'): #cand_id_list\n",
    "    '''\n",
    "    Update candidate database with any new candidates added after last candidate entry in DB\n",
    "    Key Arguments:\n",
    "    last_id -- id of last entry of the candidate database\n",
    "    '''\n",
    "    section = 'candidates?'\n",
    "    limit='100'\n",
    "    \n",
    "    #Retrieve latest candidates in Workable\n",
    "    last_id=retrieve_last_db_entry(pw=pw,db_name=db_name,user=user,host=host,port=port,database=database)\n",
    "    r_cand_upd = requests.get(url+section+'limit='+limit+'&since_id='+last_id+'.json', headers=headers)\n",
    "    if len(r_cand_upd.json()['candidates'])>1: \n",
    "        for cand in r_cand_upd.json()['candidates'][1:]: # first item in the candidates list is already in the DB\n",
    "            new_cands.append(cand['id'])\n",
    "            for k in key_list:\n",
    "                loc = locate_element(cand,k)\n",
    "                v = cand\n",
    "                for i in loc:\n",
    "                    v = v[i]\n",
    "                df_dict[k].append(v)\n",
    "    df_dict, new_cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_page(url,headers,workable_start_date='', since_id=''): #\n",
    "    '''    \n",
    "    Function to retrieve the first page in Workable\n",
    "    Input:\n",
    "    url:\n",
    "    headers:\n",
    "    workable_start_date: if want to get first page in Workable. Format: '2010-01-01T10:10:10Z'. Default: '' i.e. not specified\n",
    "    \n",
    "    Output:\n",
    "    df_dict: dictionary containing candidate data of the first page in Workable\n",
    "    '''\n",
    "    \n",
    "    # List with DataFrame columns\n",
    "    key_list = [\n",
    "        'id',\n",
    "        'name',\n",
    "        'firstname',\n",
    "        'lastname',\n",
    "        'headline',\n",
    "        'subdomain', \n",
    "        'shortcode',\n",
    "        'title',\n",
    "        'stage',\n",
    "        'disqualified',\n",
    "        'disqualification_reason',\n",
    "        'hired_at',\n",
    "        'sourced',\n",
    "        'profile_url',\n",
    "        'address',\n",
    "        'phone',\n",
    "        'email',\n",
    "        'domain',\n",
    "        'created_at',\n",
    "        'updated_at',\n",
    "    ]\n",
    "\n",
    "    # Create empty DataFrame\n",
    "    df_dict = {}\n",
    "    for key in key_list:\n",
    "        df_dict[key]=[]\n",
    "\n",
    "    cand_id_list = []\n",
    "\n",
    "    # Get first page and retrieve candidate_id\n",
    "    section = 'candidates?'\n",
    "    limit='100'\n",
    "    if workable_start_date != '':\n",
    "        created_after = '&created_after='+workable_start_date+'\n",
    "        workable_start_date = '2010-01-01T10:10:10Z'\n",
    "    else:\n",
    "        created_after = ''+\n",
    "    r_cand = requests.get(url+section+'limit='+limit+created_after.json', headers=headers)\n",
    "    for cand in r_cand.json()['candidates']:\n",
    "        cand_id_list.append(cand['id'])\n",
    "        for k in key_list:\n",
    "            loc = locate_element(cand,k)\n",
    "            v = cand\n",
    "            for i in loc:\n",
    "                v = v[i]\n",
    "            df_dict[k].append(v)\n",
    "    since_id=r_cand.json()['paging']['next'].split(\"since_id=\",1)[1]\n",
    "    return df_dict, since_id, cand_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_db_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local DB\n",
    "\n",
    "try:\n",
    "    conn = mysql.connector.connect(user='root', password='maartens1991',host='127.0.0.1', database='recruitment_dashboard')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create table REPLACE WITH CODE BELOW TO MAKE QUERY STATEMENT AND THEN EXECUTE\n",
    "    # Create sql_create_table_query = ... columns and data type\n",
    "    col_dtype = []\n",
    "    for col in df_comb.columns.tolist():\n",
    "        col_dtype.append(col+' '+str(df_comb[col].dtype))\n",
    "    \n",
    "    #Replace objects with TEXT as object is not a data type that can be parsed into CREATE TABLE statement\n",
    "    #Remove spaces in column name, replace with underscore\n",
    "    #Rename sourced to is_sourced\n",
    "    col_dtype = [s.replace('sourced', 'is_sourced') for s in col_dtype]\n",
    "    col_dtype = [s.replace('To schedule', 'To_schedule') for s in col_dtype]\n",
    "    col_dtype = [s.replace('1st Interview', '1st_Interview') for s in col_dtype]\n",
    "    col_dtype = [s.replace('2nd Interview', '2nd_Interview') for s in col_dtype]\n",
    "    col_dtype = [s.replace('object', 'TEXT') for s in col_dtype]\n",
    "    col_dtype = [s.replace('<M8[ns]', 'DATE') for s in col_dtype]\n",
    "    #if column data type is DATE, then dates have to be in the following format (as a string): YYYY-MM-DD\n",
    "    # All values in column have to be in this format, it does not take a string like 'nan'\n",
    "    # MySQL does accept NULL (not as a string, just as NULL)\n",
    "    table_col = ','.join(col_dtype)\n",
    "    \n",
    "    sql_create_table_query = \"\"\"CREATE TABLE candidates (%s)\"\"\"%(table_col)\n",
    "    cursor.execute(sql_create_table_query)\n",
    "    \n",
    "    # Save (commit) the changes\n",
    "    conn.commit()\n",
    "\n",
    "    # Select column names in order from 1st to last column\n",
    "    sql_select_query = '''SELECT column_name\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'recruitment_dashboard'\n",
    "    AND table_name   = 'candidates'\n",
    "    ORDER BY ORDINAL_POSITION\n",
    "    '''\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql_select_query)\n",
    "    records = cursor.fetchall() \n",
    "    columns = []\n",
    "    for i in records:\n",
    "        columns.append(i[0])\n",
    "    col_n = ','.join(columns)\n",
    "\n",
    "    #Records to insert in SQL table\n",
    "    records_to_insert = list(df_comb.itertuples(index=False, name=None))\n",
    "    params = ['%s' for item in records_to_insert[0]] # always use '%s' no matter the data type of the column\n",
    "    var_string = ','.join(params)\n",
    "\n",
    "    #Insert all records into table\n",
    "    sql_insert_query = \"\"\"INSERT INTO candidates (%s) VALUES (%s);\"\"\" %(col_n,var_string)\n",
    "    cursor = conn.cursor()\n",
    "    print('here')\n",
    "    cursor.executemany(sql_insert_query, records_to_insert)\n",
    "    \n",
    "    #Commit all changes\n",
    "    conn.commit()\n",
    "\n",
    "except (Exception, mysql.connector.Error) as error:\n",
    "    print (\"Error while connecting to SQL DB\", error)\n",
    "finally:\n",
    "    #Closing database connection\n",
    "    if(conn):\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"SQL DB connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Database\n",
    "\n",
    "try:\n",
    "    conn = mysql.connector.connect(user='root', password='MjB6KtDfI4pkzKr9',host='34.90.224.97', database='recruitment')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create table REPLACE WITH CODE BELOW TO MAKE QUERY STATEMENT AND THEN EXECUTE\n",
    "    # Create sql_create_table_query = ... columns and data type\n",
    "    col_dtype = []\n",
    "    for col in df_comb.columns.tolist():\n",
    "        col_dtype.append(col+' '+str(df_comb[col].dtype))\n",
    "    \n",
    "    #Replace objects with TEXT as object is not a data type that can be parsed into CREATE TABLE statement\n",
    "    #Remove spaces in column name, replace with underscore\n",
    "    #Rename sourced to is_sourced\n",
    "    col_dtype = [s.replace('sourced', 'is_sourced') for s in col_dtype]\n",
    "    col_dtype = [s.replace('To schedule', 'To_schedule') for s in col_dtype]\n",
    "    col_dtype = [s.replace('1st Interview', '1st_Interview') for s in col_dtype]\n",
    "    col_dtype = [s.replace('2nd Interview', '2nd_Interview') for s in col_dtype]\n",
    "    col_dtype = [s.replace('object', 'TEXT') for s in col_dtype]\n",
    "    table_col = ','.join(col_dtype)\n",
    "    \n",
    "    sql_create_table_query = \"\"\"CREATE TABLE candidates (%s)\"\"\"%(table_col)\n",
    "    cursor.execute(sql_create_table_query)\n",
    "    \n",
    "    # Save (commit) the changes\n",
    "    conn.commit()\n",
    "\n",
    "    # Select column names in order from 1st to last column\n",
    "    sql_select_query = '''SELECT column_name\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema = 'recruitment_dashboard'\n",
    "    AND table_name   = 'candidates'\n",
    "    ORDER BY ORDINAL_POSITION\n",
    "    '''\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(sql_select_query)\n",
    "    records = cursor.fetchall() \n",
    "    columns = []\n",
    "    for i in records:\n",
    "        columns.append(i[0])\n",
    "    col_n = ','.join(columns)\n",
    "\n",
    "    #Records to insert in SQL table\n",
    "    records_to_insert = list(df_comb.itertuples(index=False, name=None))\n",
    "    params = ['%s' for item in records_to_insert[0]] # always use '%s' no matter the data type of the column\n",
    "    var_string = ','.join(params)\n",
    "\n",
    "    #Insert all records into table\n",
    "    sql_insert_query = \"\"\"INSERT INTO candidates (%s) VALUES (%s);\"\"\" %(col_n,var_string)\n",
    "    cursor = conn.cursor()\n",
    "    print('here')\n",
    "    cursor.executemany(sql_insert_query, records_to_insert)\n",
    "    \n",
    "    #Commit all changes\n",
    "    conn.commit()\n",
    "\n",
    "except (Exception, mysql.connector.Error) as error:\n",
    "    print (\"Error while connecting to SQL DB\", error)\n",
    "finally:\n",
    "    #Closing database connection\n",
    "    if(conn):\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"SQL DB connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(url+'candidates/ebb4b0'+'.json', headers=headers).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requests.get(url+'candidates/48384b3'+'.json', headers=headers).json()\n",
    "#requests.get(url+'candidates/48384b3/activities'+'.json', headers=headers).json()['activities']\n",
    "#section = 'events?'\n",
    "#events = requests.get(url+section+'.json', headers=headers).json()\n",
    "\n",
    "#section = 'jobs/A8C5321F60/activities'\n",
    "#j = requests.get(url+section+'.json', headers=headers).json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
